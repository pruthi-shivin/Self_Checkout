{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1872 images belonging to 12 classes.\n",
      "Found 467 images belonging to 12 classes.\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/25\n",
      "59/59 [==============================] - 24s 372ms/step - loss: 1.3918 - accuracy: 0.5668 - val_loss: 0.6843 - val_accuracy: 0.7901\n",
      "Epoch 2/25\n",
      "59/59 [==============================] - 23s 393ms/step - loss: 0.7097 - accuracy: 0.7826 - val_loss: 0.5749 - val_accuracy: 0.8373\n",
      "Epoch 3/25\n",
      "59/59 [==============================] - 24s 405ms/step - loss: 0.5834 - accuracy: 0.8109 - val_loss: 0.6274 - val_accuracy: 0.8501\n",
      "Epoch 4/25\n",
      "59/59 [==============================] - 23s 395ms/step - loss: 0.5280 - accuracy: 0.8397 - val_loss: 0.5713 - val_accuracy: 0.8480\n",
      "Epoch 5/25\n",
      "59/59 [==============================] - 24s 401ms/step - loss: 0.4571 - accuracy: 0.8643 - val_loss: 0.5632 - val_accuracy: 0.8587\n",
      "Epoch 6/25\n",
      "59/59 [==============================] - 23s 392ms/step - loss: 0.4445 - accuracy: 0.8590 - val_loss: 0.5220 - val_accuracy: 0.8715\n",
      "Epoch 7/25\n",
      "59/59 [==============================] - 24s 407ms/step - loss: 0.4047 - accuracy: 0.8723 - val_loss: 0.4973 - val_accuracy: 0.8672\n",
      "Epoch 8/25\n",
      "59/59 [==============================] - 23s 395ms/step - loss: 0.3874 - accuracy: 0.8718 - val_loss: 0.5146 - val_accuracy: 0.8587\n",
      "Epoch 9/25\n",
      "59/59 [==============================] - 22s 379ms/step - loss: 0.3559 - accuracy: 0.8857 - val_loss: 0.5126 - val_accuracy: 0.8651\n",
      "Epoch 10/25\n",
      "59/59 [==============================] - 23s 383ms/step - loss: 0.3664 - accuracy: 0.8798 - val_loss: 0.5191 - val_accuracy: 0.8630\n",
      "Epoch 11/25\n",
      "59/59 [==============================] - 25s 429ms/step - loss: 0.3235 - accuracy: 0.8958 - val_loss: 0.5284 - val_accuracy: 0.8437\n",
      "Epoch 12/25\n",
      "59/59 [==============================] - 25s 430ms/step - loss: 0.2979 - accuracy: 0.8980 - val_loss: 0.5594 - val_accuracy: 0.8608\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "train_path = \"archive/My_data/train\"\n",
    "img_size = (150, 150)\n",
    "batch_size = 32\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "base_model = MobileNetV2(input_shape=(150, 150, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False \n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "model.save(\"fruit_model_mobilenet.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 637ms/step\n",
      "[00.jpeg] → Predicted: cherry, Confidence: 0.27\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[0122.jpeg] → Predicted: watermelon, Confidence: 0.80\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[22.jpeg] → Predicted: pinenapple, Confidence: 0.94\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[3.jpeg] → Predicted: avocado, Confidence: 0.50\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[321.jpeg] → Predicted: kiwi, Confidence: 0.98\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[33.jpeg] → Predicted: orange, Confidence: 0.40\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[365.jpeg] → Predicted: predict, Confidence: 0.47\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[4.jpeg] → Predicted: Apple, Confidence: 0.86\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[41.jpeg] → Predicted: kiwi, Confidence: 0.66\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[65.jpeg] → Predicted: cherry, Confidence: 0.48\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[659.jpeg] → Predicted: kiwi, Confidence: 1.00\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[66.jpeg] → Predicted: pinenapple, Confidence: 0.96\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[68.jpeg] → Predicted: predict, Confidence: 0.65\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[77.jpeg] → Predicted: pinenapple, Confidence: 0.92\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[9.jpeg] → Predicted: Banana, Confidence: 0.34\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[99.jpeg] → Predicted: pinenapple, Confidence: 0.99\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[f0.jpeg] → Predicted: watermelon, Confidence: 0.85\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[f1.jpeg] → Predicted: strawberries, Confidence: 0.75\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[f2.jpeg] → Predicted: avocado, Confidence: 0.67\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[f5.jpeg] → Predicted: predict, Confidence: 0.47\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[f7.jpeg] → Predicted: avocado, Confidence: 0.98\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[img_01.jpeg] → Predicted: avocado, Confidence: 0.59\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[img_11.jpeg] → Predicted: avocado, Confidence: 0.96\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[img_121.jpeg] → Predicted: Banana, Confidence: 0.95\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[img_141.jpeg] → Predicted: Banana, Confidence: 0.98\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[img_171.jpeg] → Predicted: Banana, Confidence: 0.98\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[img_191.jpeg] → Predicted: Banana, Confidence: 0.64\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[img_21.jpeg] → Predicted: watermelon, Confidence: 0.74\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[img_241.jpeg] → Predicted: orange, Confidence: 0.30\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[img_301.jpeg] → Predicted: Apple, Confidence: 0.60\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[img_31.jpeg] → Predicted: avocado, Confidence: 0.98\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[img_341.jpeg] → Predicted: watermelon, Confidence: 0.48\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[img_361.jpeg] → Predicted: Apple, Confidence: 0.95\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[img_371.jpeg] → Predicted: predict, Confidence: 0.60\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[img_381.jpeg] → Predicted: kiwi, Confidence: 0.75\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[img_401.jpeg] → Predicted: predict, Confidence: 0.36\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[img_41.jpeg] → Predicted: avocado, Confidence: 0.87\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[0.jpeg] → Predicted: avocado, Confidence: 0.58\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[011.jpeg] → Predicted: watermelon, Confidence: 0.49\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[012.jpeg] → Predicted: watermelon, Confidence: 0.91\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[0124.jpeg] → Predicted: watermelon, Confidence: 1.00\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[023.jpeg] → Predicted: watermelon, Confidence: 0.63\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "[1.jpeg] → Predicted: avocado, Confidence: 0.91\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[147.jpeg] → Predicted: predict, Confidence: 0.61\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[2.jpeg] → Predicted: Apple, Confidence: 0.88\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[202.jpeg] → Predicted: kiwi, Confidence: 0.98\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[img_411.jpeg] → Predicted: Apple, Confidence: 0.38\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[img_421.jpeg] → Predicted: watermelon, Confidence: 0.54\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "model = tf.keras.models.load_model(\"fruit_model_mobilenet.h5\")\n",
    "\n",
    "predict_dir = r\"C:\\Users\\pruth\\Downloads\\VIT Downloads\\Self_Checkout\\archive\\MY_data\\predict\"\n",
    "train_dir = r\"C:\\Users\\pruth\\Downloads\\VIT Downloads\\Self_Checkout\\archive\\MY_data\\train\"\n",
    "image_size = (150, 150)\n",
    "\n",
    "class_labels = sorted(os.listdir(train_dir))\n",
    "\n",
    "for item in os.listdir(predict_dir):\n",
    "    item_path = os.path.join(predict_dir, item)\n",
    "\n",
    "    if os.path.isdir(item_path):\n",
    "        for img_file in os.listdir(item_path):\n",
    "            img_path = os.path.join(item_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, image_size)\n",
    "            img = img.astype('float32') / 255.0\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "\n",
    "            prediction = model.predict(img)\n",
    "            predicted_index = np.argmax(prediction)\n",
    "            predicted_label = class_labels[predicted_index]\n",
    "            confidence = np.max(prediction)\n",
    "\n",
    "            print(f\"[{img_file}] → Predicted: {predicted_label}, Confidence: {confidence:.2f}\")\n",
    "\n",
    "    elif os.path.isfile(item_path):\n",
    "        img = cv2.imread(item_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, image_size)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        prediction = model.predict(img)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_label = class_labels[predicted_index]\n",
    "        confidence = np.max(prediction)\n",
    "\n",
    "        print(f\"[{item}] → Predicted: {predicted_label}, Confidence: {confidence:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
